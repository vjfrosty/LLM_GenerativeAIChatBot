{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from langchain import VectorDBQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import torch\n",
    "import pypdf\n",
    "import tqdm\n",
    "import sentence_transformers\n",
    "import chromadb \n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd GenerativeAIChatBot\n",
    "#$ python -m venv env\n",
    "#.\\env\\Scripts\\Activate.ps1\n",
    "#pip install langchain\n",
    "#!pip install langchain \n",
    "#!pip install InstructorEmbedding\n",
    "#!pip install torch\n",
    "#!pip install tqdm\n",
    "#!pip install sentence_transformers\n",
    "#!pip install chromadb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\work\\github\\llm_generativeaichatbot\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading chromadb-0.4.18-py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: requests>=2.28 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from chromadb) (2.31.0)\n",
      "Collecting posthog>=2.4.0\n",
      "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
      "Collecting overrides>=7.3.1\n",
      "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from chromadb) (0.15.0)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from chromadb) (2.5.2)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n",
      "Collecting onnxruntime>=1.14.1\n",
      "  Downloading onnxruntime-1.16.3-cp38-cp38-win_amd64.whl (7.4 MB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Collecting chroma-hnswlib==0.7.3\n",
      "  Downloading chroma_hnswlib-0.7.3-cp38-cp38-win_amd64.whl (150 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from chromadb) (4.8.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from chromadb) (4.66.1)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
      "Collecting fastapi>=0.95.2\n",
      "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "Collecting typer>=0.9.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Downloading mmh3-4.0.1-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Collecting pulsar-client>=3.1.0\n",
      "  Downloading pulsar_client-3.3.0-cp38-cp38-win_amd64.whl (3.4 MB)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Downloading bcrypt-4.1.1-cp37-abi3-win_amd64.whl (158 kB)\n",
      "Collecting grpcio>=1.58.0\n",
      "  Downloading grpcio-1.59.3-cp38-cp38-win_amd64.whl (3.7 MB)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from chromadb) (1.24.4)\n",
      "Collecting pypika>=0.48.9\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting graphlib-backport>=1.0.3\n",
      "  Downloading graphlib_backport-1.0.3-py3-none-any.whl (5.1 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.2.0)\n",
      "Collecting urllib3<2.0,>=1.24.2\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-1.6.4-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Downloading google_auth-2.24.0-py2.py3-none-any.whl (183 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting requests-oauthlib\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.1-cp38-cp38-win_amd64.whl (413 kB)\n",
      "Requirement already satisfied: sympy in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: packaging in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
      "Collecting flatbuffers\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.9.0)\n",
      "Collecting deprecated>=1.2.6\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-win_amd64.whl (37 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-proto==1.21.0\n",
      "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
      "Collecting backoff<3.0.0,>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting googleapis-common-protos~=1.52\n",
      "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.42b0\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.42b0\n",
      "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
      "Collecting opentelemetry-instrumentation==0.42b0\n",
      "  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n",
      "Collecting opentelemetry-util-http==0.42b0\n",
      "  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (56.0.0)\n",
      "Collecting asgiref~=3.0\n",
      "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.14.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from requests>=2.28->chromadb) (3.3.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.12.0)\n",
      "Requirement already satisfied: filelock in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: colorama in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from tqdm>=4.65.0->chromadb) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting httptools>=0.5.0\n",
      "  Downloading httptools-0.6.1-cp38-cp38-win_amd64.whl (60 kB)\n",
      "Collecting watchfiles>=0.13\n",
      "  Downloading watchfiles-0.21.0-cp38-none-win_amd64.whl (279 kB)\n",
      "Collecting websockets>=10.4\n",
      "  Downloading websockets-12.0-cp38-cp38-win_amd64.whl (124 kB)\n",
      "Collecting python-dotenv>=0.13\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\work\\github\\llm_generativeaichatbot\\env\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (PEP 517): started\n",
      "  Building wheel for pypika (PEP 517): finished with status 'done'\n",
      "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53836 sha256=b038f7552d367316df7c0cdacfdff1d8a22b7ee5fa7deec50ea5962503d122d1\n",
      "  Stored in directory: c:\\users\\yasen\\appdata\\local\\pip\\cache\\wheels\\54\\4a\\f8\\2803c6041841502d0d21fb2a62d401d14716dfeb2261a3a70b\n",
      "Successfully built pypika\n",
      "Installing collected packages: wrapt, deprecated, urllib3, pyreadline3, pyasn1, protobuf, opentelemetry-api, rsa, pyasn1-modules, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, opentelemetry-instrumentation, oauthlib, humanfriendly, h11, cachetools, backoff, asgiref, websockets, websocket-client, watchfiles, uvicorn, starlette, requests-oauthlib, python-dotenv, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-common, monotonic, httptools, grpcio, googleapis-common-protos, google-auth, flatbuffers, coloredlogs, typer, pypika, pulsar-client, posthog, overrides, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, onnxruntime, mmh3, kubernetes, importlib-resources, graphlib-backport, fastapi, chroma-hnswlib, bcrypt, chromadb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "Successfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.1 cachetools-5.3.2 chroma-hnswlib-0.7.3 chromadb-0.4.18 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.104.1 flatbuffers-23.5.26 google-auth-2.24.0 googleapis-common-protos-1.61.0 graphlib-backport-1.0.3 grpcio-1.59.3 h11-0.14.0 httptools-0.6.1 humanfriendly-10.0 importlib-resources-6.1.1 kubernetes-28.1.0 mmh3-4.0.1 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.16.3 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.0.2 protobuf-4.25.1 pulsar-client-3.3.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 pypika-0.48.9 pyreadline3-3.4.1 python-dotenv-1.0.0 requests-oauthlib-1.3.1 rsa-4.9 starlette-0.27.0 typer-0.9.0 urllib3-1.26.18 uvicorn-0.24.0.post1 watchfiles-0.21.0 websocket-client-1.6.4 websockets-12.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Work\\GitHub\\LLM_GenerativeAIChatBot\\env\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "c:\\Work\\GitHub\\LLM_GenerativeAIChatBot\\env\\lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:253: UserWarning: `VectorDBQA` is deprecated - please use `from langchain.chains import RetrievalQA`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readability, elegant, detailed, focused, undistracted\n",
      "a mess\n"
     ]
    }
   ],
   "source": [
    "def get_pdf_text(filePath):\n",
    "    loader = PyPDFLoader(filePath)\n",
    "    doc = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1500, separator=\"\\n\")\n",
    "    chunks = text_splitter.split_documents(doc)\n",
    "    return chunks\n",
    "\n",
    "# Turn document to chunks\n",
    "\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "# Convert to vector db\n",
    "\n",
    "\n",
    "def get_vectorDb(text_chunks):\n",
    "    embeddings = HuggingFaceInstructEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "    vectorDb = Chroma.from_documents(text_chunks, embeddings)\n",
    "    return vectorDb\n",
    "\n",
    "\n",
    "def get_conversation_chain(vectorDb, largeLangModel):\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', return_messages=True)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=largeLangModel,\n",
    "        retriever=vectorDb.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    return conversation_chain\n",
    "\n",
    "\n",
    "def main():\n",
    "    filePath = 'resources/cleanCode.pdf'\n",
    "    pdf_text = get_pdf_text(filePath)\n",
    "    # text_chunks = get_text_chunks(pdf_text)\n",
    "    vectorDb = get_vectorDb(pdf_text)\n",
    "    largeLangModel = HuggingFaceHub(\n",
    "        repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\": 0.5, \"max_length\": 1024})\n",
    "    qa = VectorDBQA.from_chain_type(\n",
    "        llm=largeLangModel, chain_type=\"stuff\", vectorstore=vectorDb)\n",
    "    query = \"What is clean code?\"\n",
    "    result = qa.run(query)\n",
    "    print(result)\n",
    "    query2 = \"What is bad code?\"\n",
    "    result2 = qa.run(query2)\n",
    "    print(result2)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
